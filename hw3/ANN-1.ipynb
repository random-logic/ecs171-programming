{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Iris.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 169\u001b[0m\n\u001b[1;32m    165\u001b[0m     y \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[0;32m--> 169\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIris.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m model \u001b[38;5;241m=\u001b[39m Network()\n\u001b[1;32m    172\u001b[0m model\u001b[38;5;241m.\u001b[39madd(DenseLayer(\u001b[38;5;241m6\u001b[39m))\n",
      "Cell \u001b[0;32mIn[1], line 157\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(path):\n\u001b[0;32m--> 157\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    160\u001b[0m     target \u001b[38;5;241m=\u001b[39m cols\u001b[38;5;241m.\u001b[39mpop()\n",
      "File \u001b[0;32m~/anaconda3/envs/ecs171/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ecs171/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/ecs171/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ecs171/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/ecs171/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Iris.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, neurons):\n",
    "        # Initialize the layer with the given number of neurons\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def relu(self, inputs):\n",
    "        # Implement the ReLU activation function on the given inputs\n",
    "        return np.maximum(0, inputs)\n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        # Implement the softmax activation function on the given inputs\n",
    "        exp_scores = np.exp(inputs)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "    def relu_derivative(self, dA, N):\n",
    "        # Calculate the derivative of the cost function with respect to the inputs of the layer using the derivative of the ReLU activation function\n",
    "        # dA is the derivative of the cost function with respect to the output of the layer\n",
    "        # N is the weighted sum of the inputs to the layer\n",
    "        # First create a binary vector that is 1 where the input to the ReLU function was positive and 0 where it was negative or zero\n",
    "        dN = (N <= 0).astype(float)\n",
    "        # Multiply dN element-wise with dA to obtain the derivative of the cost function with respect to the output of the ReLU function\n",
    "        return dN * dA\n",
    "    \n",
    "    def forward(self, inputs, weights, bias, activation):\n",
    "        N = (inputs @ weights.T) + bias\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            A = self.relu(N)\n",
    "        elif activation == 'softmax':\n",
    "            A = self.softmax(N)\n",
    "            \n",
    "        return A, N\n",
    "    \n",
    "    def backward(self, dA_curr, W_curr, Z_curr, A_prev, activation):\n",
    "        if activation == 'softmax':\n",
    "            dW = A_prev.T @ dA_curr\n",
    "            db = np.sum(dA_curr, axis=0, keepdims=True)\n",
    "            dA = dA_curr @ W_curr\n",
    "        else:\n",
    "            dZ = self.relu_derivative(dA_curr, Z_curr)\n",
    "            dW = A_prev.T @ dZ\n",
    "            db = np.sum(dZ, axis=0, keepdims=True)\n",
    "            dA = dZ @ W_curr\n",
    "            \n",
    "        return dA, dW, db\n",
    "\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.network = []\n",
    "        self.architecture = []\n",
    "        self.params = []\n",
    "        self.memory = []\n",
    "        self.gradients = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network.append(layer)\n",
    "            \n",
    "    def _compile(self, data):\n",
    "        for idx, layer in enumerate(self.network):\n",
    "            if idx == 0:\n",
    "                self.architecture.append({'input_dim':data.shape[1], 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'relu'})\n",
    "            elif idx > 0 and idx < len(self.network)-1:\n",
    "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'relu'})\n",
    "            else:\n",
    "                self.architecture.append({'input_dim':self.network[idx-1].neurons, 'output_dim':self.network[idx].neurons,\n",
    "                                         'activation':'softmax'})\n",
    "        return self\n",
    "    \n",
    "    def _init_weights(self, data):\n",
    "        self._compile(data)\n",
    "        \n",
    "        np.random.seed(99)\n",
    "        \n",
    "        for layers in self.architecture:\n",
    "            self.params.append({\n",
    "                'W':np.random.uniform(low=-1, high=1, size=(layers['output_dim'], layers['input_dim'])),\n",
    "                'b':np.zeros((1, layers['output_dim']))})\n",
    "        return self\n",
    "    \n",
    "    def _forwardprop(self, data):\n",
    "        A_curr = data.copy()\n",
    "        \n",
    "        for i in range(len(self.params)):\n",
    "            A_prev = A_curr\n",
    "            A_curr, Z_curr = self.network[i].forward(inputs=A_prev, weights=self.params[i]['W'], \n",
    "                                           bias=self.params[i]['b'], activation=self.architecture[i]['activation'])\n",
    "            \n",
    "            self.memory.append({'inputs':A_prev, 'Z':Z_curr})\n",
    "            \n",
    "        return A_curr\n",
    "    \n",
    "    def _backprop(self, predicted, actual):\n",
    "        num_samples = len(actual)\n",
    "        \n",
    "        dscores = predicted\n",
    "        dscores[range(num_samples),actual] -= 1\n",
    "        dscores /= num_samples\n",
    "        \n",
    "        dA_prev = dscores\n",
    "        \n",
    "        for idx, layer in reversed(list(enumerate(self.network))):\n",
    "            dA_curr = dA_prev\n",
    "            \n",
    "            A_prev = self.memory[idx]['inputs']\n",
    "            Z_curr = self.memory[idx]['Z']\n",
    "            W_curr = self.params[idx]['W']\n",
    "            \n",
    "            activation = self.architecture[idx]['activation']\n",
    "\n",
    "            dA_prev, dW_curr, db_curr = layer.backward(dA_curr, W_curr, Z_curr, A_prev, activation)\n",
    "\n",
    "            self.gradients.append({'dW':dW_curr, 'db':db_curr})\n",
    "            \n",
    "    def _update(self, lr=0.01):\n",
    "        for idx, _ in enumerate(self.network):\n",
    "            self.params[idx]['W'] -= lr * list(reversed(self.gradients))[idx]['dW'].T  \n",
    "            self.params[idx]['b'] -= lr * list(reversed(self.gradients))[idx]['db']\n",
    "    \n",
    "    def _get_accuracy(self, predicted, actual):\n",
    "        return np.mean(np.argmax(predicted, axis=1)==actual)\n",
    "    \n",
    "    def _calculate_loss(self, predicted, actual):\n",
    "        samples = len(actual)\n",
    "        \n",
    "        correct_logprobs = -np.log(predicted[range(samples),actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def train(self, X_train, y_train, epochs):\n",
    "        self.loss = []\n",
    "        self.accuracy = []\n",
    "        \n",
    "        self._init_weights(X_train)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            yhat = self._forwardprop(X_train)\n",
    "            self.accuracy.append(self._get_accuracy(predicted=yhat, actual=y_train))\n",
    "            self.loss.append(self._calculate_loss(predicted=yhat, actual=y_train))\n",
    "            \n",
    "            self._backprop(predicted=yhat, actual=y_train)\n",
    "            \n",
    "            self._update()\n",
    "            \n",
    "            if i % 20 == 0:\n",
    "                s = 'EPOCH: {}, ACCURACY: {}, LOSS: {}'.format(i, self.accuracy[-1], self.loss[-1])\n",
    "                print(s)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    def get_data(path):\n",
    "        data = pd.read_csv(path, index_col=0)\n",
    "\n",
    "        cols = list(data.columns)\n",
    "        target = cols.pop()\n",
    "\n",
    "        X = data[cols].copy()\n",
    "        y = data[target].copy()\n",
    "\n",
    "        y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    X, y = get_data(\"Iris.csv\")\n",
    "\n",
    "    model = Network()\n",
    "    model.add(DenseLayer(6))\n",
    "    model.add(DenseLayer(8))\n",
    "    model.add(DenseLayer(10))\n",
    "    model.add(DenseLayer(3))\n",
    "\n",
    "    model.train(X_train=X, y_train=y, epochs=200)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the relu function\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "#f'(x) = 1 if x > 0\n",
    "#f'(x) = 0 if x <= 0\n",
    "def relu_derivative(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# Define the sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "#f'(x) = f(x) * (1 - f(x))\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Define the residual sum of squares loss function\n",
    "def loss(Y_hat, Y):\n",
    "    return np.sum(np.square(Y_hat - Y))\n",
    "\n",
    "# Define the cross-entropy loss function\n",
    "#def loss(Y_hat, Y):\n",
    "#    return -np.mean(np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat), axis=1))\n",
    "\n",
    "# Initialize the weights and biases\n",
    "def initialize_params(num_features, num_classes):\n",
    "    W1 = np.random.randn(num_features, 4) * 0.01\n",
    "    b1 = np.zeros((1, 4))\n",
    "    W2 = np.random.randn(4, num_classes) * 0.01\n",
    "    b2 = np.zeros((1, num_classes))\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Define the forward propagation function\n",
    "def forward_prop(X, W1, b1, W2, b2):\n",
    "    # Calculate the weighted sum of the input features and the weights of the first layer\n",
    "    Z1 = np.dot(X, W1) + b1\n",
    "    # Apply the rectified linear unit (ReLU) activation function to the first layer's output\n",
    "    A1 = relu(Z1)\n",
    "    # Calculate the weighted sum of the first layer's output and the weights of the output layer\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    # Apply the sigmoid activation function to the output layer's output\n",
    "    A2 = sigmoid(Z2)\n",
    "    # Return the output of the output layer (A2), as well as the output and weighted sum of the first layer (A1 and Z1)\n",
    "    return A2, A1, Z1\n",
    "\n",
    "# Define the backpropagation function\n",
    "def backprop(X, Y, Y_hat, lr, A1, Z1, W1, W2, b1, b2):\n",
    "    # Calculate the error in the output layer using the derivative of the sigmoid activation function\n",
    "    delta2 = (Y_hat - Y) * (Y_hat * (1 - Y_hat))\n",
    "    # Calculate the gradients of the weights and biases in the output layer using the error and the output of the first hidden layer\n",
    "    dW2 = np.dot(A1.T, delta2)\n",
    "    db2 = np.sum(delta2, axis=0, keepdims=True)\n",
    "    # Calculate the error in the first hidden layer using the derivative of the ReLU activation function and the gradients of the weights in the output layer\n",
    "    delta1 = np.dot(delta2, W2.T) * (Z1 > 0)\n",
    "    # Calculate the gradients of the weights and biases in the first hidden layer using the error and the input data\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    db1 = np.sum(delta1, axis=0)\n",
    "    # Update weights and biases using the calculated gradients and the learning rate\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    # Return the updated weights and biases\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, Y = iris.data, iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y = encoder.fit_transform(Y)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the weights and biases\n",
    "W1, b1, W2, b2 = initialize_params(X.shape[1], Y.shape[1])\n",
    "\n",
    "# Train the model using backpropagation\n",
    "num_epochs = 10000\n",
    "lr = 0.01\n",
    "print_freq = 500\n",
    "W1_shapes = []\n",
    "b1_shapes = []\n",
    "W2_shapes = []\n",
    "b2_shapes = []\n",
    "loss_curve = []\n",
    "for i in range(num_epochs):\n",
    "    # Forward propagation\n",
    "    Y_hat, A1, Z1 = forward_prop(X_train, W1, b1, W2, b2)\n",
    "    cost = loss(Y_hat, Y_train)\n",
    "    loss_curve.append(cost)\n",
    "    # Backpropagation\n",
    "    W1, b1, W2, b2 = backprop(X_train, Y_train, Y_hat, lr, A1, Z1, W1, W2, b1, b2)\n",
    "    # Store shapes of weights and biases\n",
    "    if i % print_freq == 0:\n",
    "        W1_shapes.append(W1)\n",
    "        b1_shapes.append(b1)\n",
    "        W2_shapes.append(W2)\n",
    "        b2_shapes.append(b2)\n",
    "        print(f\"Epoch {i}: Loss = {cost:.4f}\")\n",
    "        print(f\"W1 = {W1}\")\n",
    "        print(f\"b1 = {b1}\")\n",
    "        print(f\"W2 = {W2}\")\n",
    "        print(f\"b2 = {b2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final cost\n",
    "final_cost = loss_curve[-1]\n",
    "print(f\"Final cost: {final_cost:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(range(num_epochs), loss_curve)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ReLU activation function and its gradient\n",
    "x = np.linspace(-5, 5, 100)\n",
    "plt.plot(x, relu(x), label='ReLU')\n",
    "plt.plot(x, relu_derivative(x), label='ReLU gradient')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('ReLU activation function and its gradient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = sigmoid(x)\n",
    "dy = sigmoid_derivative(x)\n",
    "\n",
    "plt.plot(x, y, label=\"sigmoid\")\n",
    "plt.plot(x, dy, label=\"sigmoid derivative\")\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('ReLU activation function and its gradient')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
